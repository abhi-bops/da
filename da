#!/usr/bin/python3

#NOTE:
#This is a flat file genereated by using the da_* scripts, if you want to modify 
# clone the repository https://github.com/abhi-bops/da and then edit the files 
# and then run get_daflat.py to get a flat file like this. 
# The sole purpose of this file is to genereate the python script da, a single file 
# that can be copied and run without worrying about importing the other da_* files.

from math import nan, ceil, floor, isnan
import statistics as stats
from io import BytesIO #Convert image into bytes
import base64 #For image base64 code
#For running commands in shell 
import shlex 
import subprocess
from importlib import import_module
from itertools import chain
from collections import defaultdict

def check_module(module_name, install=False):
    try: 
        import_module(module_name)
    except ModuleNotFoundError:
        if install:
            curldl = subprocess.Popen('curl https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py -s'.split(' '))
            o = curldl.communicate()
            if o[1] != None:
                return False
            from pathlib import Path
            home = str(Path.home())
            runpip = subprocess.Popen('python3 /tmp/get-pip.py --user'.split(' '))
            o = curldl.communicate()
            if o[1] != None:
                return False
            pipinstall = subprocess.Popen("{}/.local/bin/pip3 install {}".format(home, module).split(' '))
            if o[1] != None:
                return False
            return True
        else:
            return False
    else:
        return True

def imgcat(b64):
    """To print the image on the terminal screen, works for iterm only"""
    start='\033]1337;File=inline=1:'
    b64s = b64.decode(encoding='utf-8')    
    end = '\a\n'
    img_data = start+b64s+end
    cmd = shlex.split('printf {}'.format(img_data))
    subprocess.call(cmd)
    subprocess.call('echo')

def is_empty(figure):
    """
    Return whether the figure contains no Artists (other than the default
    background patch).
    https://matplotlib.org/faq/howto_faq.html#check-whether-a-figure-is-empty
    """
    contained_artists = figure.get_children()
    return len(contained_artists) <= 1

def to_b64(fig, dpi=100):
    """
    Convert the png image file into base64 encoded data
    
    Input
    -----
    fig: Figure 
       Figure object from matplotlib.figure.Figure

    dpi: int
       Dots per inch for the image

    Output
    ------
    binary
       Base64 encoded data of the image
    """
    if is_empty(fig):
        return None
    with BytesIO() as img_io:
        fig.savefig(img_io, format='png', dpi=dpi, bbox_inches='tight')
        b64 = base64.b64encode(img_io.getvalue())
    return b64

def get_argmax():
    """Get the maximum length of arguments that can be passed to a command"""
    command = subprocess.Popen(['getconf', 'ARG_MAX'], stdout=subprocess.PIPE)
    o = command.communicate()
    return int(o[0].decode('utf-8').strip('\n'))
    
##Custom functions for transform
def f_share(data, other=None):
    total = sum(filter(None, map(convert_float, data)))
    out = []
    for i in data:
        try:
            share = round(float(i)/total, 2)
            if other=='g':
                width = 20
                barlength = int(width*share)
                share = '{:>5} |{:<{width}}'.format(str(share),"o"*barlength, width=width)
            out.append(share)
        except ValueError:
            out.append('-')
    return out

def f_cumsum(data, other=None):
    sum_tmp = 0
    out = []
    for i in data:
        sum_tmp += i        
        out.append(sum_tmp)
        print(i, sum_tmp)
    return out

## Helpful functions
def convert_float(x):
    """
    If a string can be converted into float return the float number, else return None
    """
    try:
        out = float(x)
    except ValueError:
        return None
    else:
        return out
    
def get_uniq_fields(fields):
    """
    Remove duplicate fields. Preserves the order, only the first occurence is considered.
    
    Parameters
    ----------
    fields: list
       list of items, can have repeating items

    Retruns
    -------
    list
       list of unique items
    """
    need_fields = []
    for i in fields:
        if i not in need_fields:
            need_fields.append(i)    
    return need_fields

def transpose_rows(rows):
    """
    Transpose a 2D matrix
    input:
    1 2 3
    4 5 6
    
    output:
    1 4
    2 5
    3 6

    Parameters
    ----------
    rows: list
       list of rows (list) of data
    
    Returns
    -------
    list
       list of columns (list) of data
    """
    if not rows:
        return []
    rows = list(rows)
    length = len(rows[0])
    return [list(map(lambda x:x[i], rows)) for i in range(length)]

def f_aggfunc(data, aggfunc, need_sort=False):
    """
    Define functions to run on a list.
    Available: first, last, concat, max, min, sum, count, 
               mean, average, avg, median, p50, pN (N is any interger (0->100),
               stddev
    Any other agg function returns a None

    Parameters
    ----------
    data: list
       list of items

    aggfunc: str
       function to run, if function is not in the list, it returns None

    need_sort: Bool
       Some of the operations expect data to be sorted, this option can be used if needed.

    Returns
    -------
    Single element (int/str/float)
       Runs the aggregation function and retursn the result
    """
    #If there is no value, return None
    if not data or data == [''] or data == []:
        return None
    if aggfunc == 'first':
        return data[0]
    if aggfunc == 'last':
        return data[-1]
    if aggfunc == 'concat':
        return ' '.join(data)
    #Handle aggfunc is None, return the first data point
    if not aggfunc:
        return data[0]
    #For numerical functions convert data into float
    data = list(filter(lambda x:x is not nan, map(convert_float, data)))
    #Remove the None
    data = list(filter(None, data))
    #Sort the data if it was needed
    if need_sort:
        data = sorted(data)
    if not data:
        return None
    if aggfunc == 'max':
        return max(data)
    if aggfunc == 'min':
        return min(data)
    if aggfunc == 'sum':
        return round(sum(data), 2)
    if aggfunc == 'count':
        return len(data)
    if aggfunc in ('mean', 'average', 'avg'):
        return round(sum(data)/len(data), 2)
    if aggfunc in ('median', 'p50'):
        return stats.median(data)
    if aggfunc.startswith('p'):
        p = int(aggfunc[1:])/100
        return data[int(p*len(data))]
    if aggfunc in ('stddev'):
        return round(stats.pstdev(data), 2)
    #If no matches return None
    return None

def get_transform_req(t_input):
    #formatting pattern for the transform request
    t_format = 'fieldN:func:params|chain_func:params2=alias'
    #fieldN -> field number on which the transformation will be done
    #function:params -> first pass transformation and parameters to the function
    #|chain_func:params -> second pass transformation and parameters to the function.
    #                      can be multiple functions separated by "|"
    #=alias -> column name to use
    
    #Append transform actions in order into the list
    t_list = []
    #t_input is a list of transform requests in the format t_format
    # translate that into a list of dictionaries with options abd values as key-value pairs
    for t_req in t_input:
        t_req, sep, t_alias = t_req.partition('=')
        t_req, sep, t_chain = t_req.partition('|')
        chain_l = []        
        if t_chain:
            sep = '|'
            chain_fmt = 'func:params'
            while sep:
                chain_1, sep, chain_2 = t_chain.partition('|')
                chain_d = dict(zip_longest(chain_fmt.split(':'), chain_1.split(':')))
                #The params option can be a field value, or a constant.
                if chain_d['params'].startswith('f'):
                    chain_d['params'] = int(chain_d['params'][1:])
                    chain_d['is_field']= True
                else:
                    chain_d['params'] = float(chain_d['params'])
                    chain_d['is_field'] = False
                chain_l.append(chain_d)
                t_chain = chain_2
        t_options = dict(enumerate(t_req.split(':')))
        #Check for each option and ensure it has a default value
        fields = [int(t_options.get(0, '0').strip('f'))]
        func = t_options.get(1, None)
        params = t_options.get(2, '1')        #The last option can be a field value, or a constant.
        if params.startswith('f'):
            params = int(params[1:])
            is_field= True
        else:
            #params = float(params)
            is_field = False
        t_list.append({'fields': fields, 'func': func, 'params': params, 'is_field': is_field, 'alias': t_alias, 'chain': chain_l})
    return t_list

def get_fields(fields):
    """
    function to parse the input field format and return all the fields that was requested
    If numbers are separated by '-'; get the range of input
    if separated by ','; get the numbers
    """
    #Extract field numbers and clean
    field_l = filter(None, map(lambda x:x.strip(), fields.split(',')))
    all_fields = []
    for f in field_l:
        if '-' in f:
            start, _, end = f.partition('-')
            all_fields += list(range(int(start), int(end)+1))
        elif f:
            all_fields.append(int(f))
    return all_fields

#Histogram related functions
def create_if_not_exists(d, i, t=None):
    """
    Create a key `i` in dictionary `d` with the value `t`, if `i` does not exist.
    """
    if not d.get(i):
        if t == None:
            t = []
        d[i] = t
        
def round_to_factor(number, factor):
    """Find the nearest multiple of factor <= number"""
    return int(number/factor)*factor

def create_bins(bin_min, bin_max, bin_size=10, bin_count=0, round_value=1):
    """
    Create bins for histogram.
    
    Parameters
    ----------
    bin_min: int
        Start of the bin

    bin_max: int
        Final value of the bin

    bin_size: int
        Size of each bin

    bin_count: int
        Number of bins needed

    round_value: int
        Precision of bins

    Returns
    -------
    list
       list of bin values that can be used to bucket data
    """
    #Keeping the min-bin and max-bin boundaries as integers, for readability
    #Since bin_size is also an integer, all bin boundaries will be integers
    if bin_count > 0:
        bin_size = floor((bin_max - bin_min)/bin_count)
    #starting bin should be floor of the decimal passed
    bin_now = floor(bin_min)
    bins = []
    while True: 
        if bin_now >= bin_max: 
            bin_now = bin_max 
            #bins.append(round_to_factor(bin_now, round_value))
            #Last bin should be the ceil of max
            bins.append(ceil(bin_now))
            break 
        else:
            bins.append(bin_now)
            #bins.append(round_to_factor(bin_now, round_value))
            bin_now += bin_size
    return bins

def get_result(data, minv=None, maxv=None, count=20, bin_size=None, bins=[]):
    """
    Create histogram bins
    """
    #Sort the data
    data.sort()
    #min data point for binning
    bin_min = minv or data[0]
    #Max data point for binning
    bin_max = maxv or data[-1]
    #Computation of histogram boundaries, precdence bins > size > count
    #1. bins -> list of values to act as right end of bins
    #2. size -> compute bins based on size of individual bin
    #3. count -> compute bins based on count of bins needed (fallback)
    if bins:
        bins.sort()
        bin_max = bins[-1]
        bin_min = bins[0]
    elif bin_size:
        bins = create_bins(bin_min=bin_min,
                           bin_max=bin_max,
                           bin_size=bin_size)
    else:
        bin_count = count
        if bin_count > (bin_max-bin_min):
            bin_count = bin_max-bin_min
        bins = create_bins(bin_min=bin_min,
                           bin_max=bin_max,
                           bin_count=bin_count)
    if data[-1] > bin_max:
        bins = bins + [data[-1]]
    counter = 0
    bin_d = {}
    for i in data:
        create_if_not_exists(bin_d, bins[counter], 0)
        #X->i (inclusive)
        if i <= bins[counter]: 
            bin_d[bins[counter]] += 1 
        else: 
            found = False
            while found == False:
                counter += 1
                create_if_not_exists(bin_d, bins[counter], 0)
                if i <= bins[counter]:
                    bin_d[bins[counter]] += 1
                    found = True
    i_old = bin_min-1
    return bin_d

def rich_print_table(data, heading, repeat_heading=50, title=None, table_out=False, justify={0: 'left'}):
    rich_check = check_module('rich', install=False)
    if rich_check:
        from rich.console import Console
        from rich.table import Table
        from rich import box
    else:
        return None
    table = Table(title=title, box=box.DOUBLE_EDGE)
    for n, h in enumerate(heading):
        table.add_column(str(h), justify=justify.get(n, 'right'), header_style='bold green',
                         overflow='fold', no_wrap=False)
    row_counter = 0
    for d in data:
        d = map(str, d)
        row_counter += 1
        if row_counter%repeat_heading == 0:
            table.add_row(*d, end_section=True )
            table.add_row(*heading, end_section=True, style="bold green")
        else:
            table.add_row(*d)
    if table_out:
        return table
    console = Console()
    with console.capture() as capture:
            console.print(table)
    str_output = capture.get()
    return str_output

    
def rich_print_layout(rows, columns, data_l):
    rich_check = check_module('rich', install=False)
    if rich_check:
        from rich.layout import Layout
        from rich import print
        from rich.console import Console
    else:
        return None
    layout = Layout()
    row_layout = []
    col_layout = defaultdict(list)
    for r in range(rows):
        row_layout.append(Layout(name="r{}".format(r)))
        for c in range(columns):
            col_layout['r{}'.format(r)].append(Layout(name="c{}{}".format(r, c)))
    layout.split(*row_layout)
    for i in range(len(row_layout)):
        layout['r{}'.format(i)].split(*col_layout['r{}'.format(i)], direction='horizontal')
    col_layout_l = list(chain.from_iterable([v for v in col_layout.values()]))
    for n, d in enumerate(data_l):
        table = rich_print_table(d['data'], d['heading'])
        col_layout_l[n].update(table)
    console = Console()
    with console.capture() as capture:
            console.print(layout)
    str_output = capture.get()
    print(str_output)

    
    return layout
        


import fileinput
from math import nan, ceil, floor, isnan
from itertools import tee, starmap, repeat
import statistics as stats
from collections import defaultdict, Counter
#Ignore warnings that are thrown by matplotlib/pandas
from warnings import filterwarnings
filterwarnings('ignore')
import re
import os

#Check if we have the necessary imports for graphing
global is_graphing
is_graphing=True
try:
    import pandas as pd, numpy as np, matplotlib, seaborn as sns
    from matplotlib.figure import Figure
    import matplotlib.ticker as ticker #For formatting log scales
    large = 20; med = 16; med2 = 14; small = 10
    params = {'axes.titlesize': large,
              'legend.fontsize': med,
              'axes.labelsize': med2,
              'axes.titlesize': med2,
              'xtick.labelsize': med,
              'ytick.labelsize': med,
              'figure.titlesize': large,
              'figure.facecolor':'#eceff4'}
    matplotlib.rcParams.update(params)
    matplotlib.style.use('seaborn')
    sns.set(rc=params)
    #Colors are from https://www.nordtheme.com/docs/colors-and-palettes
    calmjet = matplotlib.colors.LinearSegmentedColormap.from_list("", [ "#d8dee9","#81a1c1", "#a3be8c", "#bf616a"])

except ImportError:
    if force_install:
        pandas_check = check_module('pandas', install=True)
        matplotlib_check = check_module('matplotlib', install=True)
        seaborn_check = check_module('seaborn', install=True)
        if pandas_check and matplotlib_check and seaborn_check and numpy:
            import pandas as pd, matplotlib, seaborn as sns
            from matplotlib.figure import Figure
            import matplotlib.ticker as ticker #For formatting log scales            
            is_graphing = True
    else:
        is_graphing=False

global missing_char
missing_char = '-'

class Graph(object):
    def __init__(self, data, heading=None, **kwargs):
        #Parse the graphing options, pop everything that is not graph specific
        # Remaing ones are passed to graphs, expecting them to be graph specific
        self.x = int(kwargs.pop('x', 0)) #column for x-axis
        self.y = kwargs.pop('y', [1])
        #self.hue = kwargs.pop('hue', None) #column to use to distinguish multiple series
        self.hue = None
        self.is_ts = kwargs.pop('is_ts', False) #If axis is a timestamp
        self.kind = kwargs.pop('kind', 'bar') #Graph type
        self.aggfunc = kwargs.pop('aggfunc', 'mean') #If multiple values for a combination of x, hue is present. Use this to aggregate 
        self.split_interval = int(kwargs.pop('split_interval', 50) or 50) #To split the graphs, so that it does not hit the bash arg_max limits
        self.subplots = kwargs.pop('subplots', True)
        self.title = kwargs.pop('title', 'Graph')
        self.missing_char = kwargs.pop('missing_char', '-')
        terminalcols = os.get_terminal_size()[0]
        self.dpi = terminalcols - 20
        #If data is a dict, set heading to None
        self.data = data
        if isinstance(data, (dict)):
            self.heading = None
        else:
            self.heading = heading
        self.data_prep()
        self.handle_timeindex()
        self.get_canvas()

    def data_prep(self):
        #Create the dataframe
        df = pd.DataFrame(data=self.data, columns=self.heading)
        self.shape = df.shape
        #If the graph is not 'scatter' and x is given > 0, then treat x as index
        if self.kind != 'scatter' and self.x>=0:
            #Get the index column out
            index = df.columns[self.x]
            df.index = df[index]
            #Select only the y columns
            df = df.iloc[:, self.y]
        #Change all missing_char to nan
        df.replace(self.missing_char, nan, inplace=True)
        #Change datatypes of values to float, so that plotting functions can treat them as numbers
        self.df = df.astype('float')

    def handle_timeindex(self):
        #Handling timeindexes
        #If we need to treat the index as a time value
        if self.is_ts == True:
            # If the 10 characters are integers AND optionally can include "." and any numerical characters
            # This matches unix timestamp as seconds AND also as seconds.milliseconds            
            if re.fullmatch('[0-9]{10}(\.[0-9]+)?', self.df.index[0]):
                self.df.index = pd.to_datetime(self.df.index, unit='s', errors='coerce')
            #Otherwise let pandas try it
            else:
                self.df.index = pd.to_datetime(self.df.index, infer_datetime_format=True, errors='coerce')
            #Indicate the rows that were dropped
            unmatched = self.df[self.df.index == 'Nat']
            if len(unmatched) > 0:
                print("Unmatched data that was dropped for plotting, items={}".format(len(unmatched)))
            self.df = self.df[~(self.df.index == 'NaT')]

    def get_canvas(self):
        #Plotting, need to use optional time units if needed
        #Define a figure class
        self.w = min(30, max(15, self.shape[0]/10))
        #If subplots is True is used, accomodate for each subplot
        # for box, pandas can handle side-by-side plots, so no need
        self.h = 6*len(self.y) if self.subplots and self.kind != 'box' else 6
        #Swap if it's barh
        if self.kind == 'barh': 
            self.w, self.h = self.h, self.w
            #Somehow if the graph is bigger than this, shell breaks due to a large argument list,
            # 18 was got by trial and error method - maybe can change based on elements in graph
            self.h = max(18, self.h)
        elif self.kind == 'heatmap':
            self.w = 20
            self.h = 6

    def to_b64(self, fig, dpi=100):
        out = to_b64(fig, dpi)
        return out

    def get_max(self):
        return self.df.max().max()

    def get_min(self):
        return self.df.min().max()

    #Formatting for the x-axis/time axis
    def format_x(self, x, pos=None):
        if self.is_ts == True:
            #If index is a timeindex format it show HH:MM:SS            
            return self.df.index[int(x)].strftime('%H:%M:%S')
        else:
            return self.df.index[int(x)]
        
    def plot(self, **kwargs):
        """
        Plot the graphs
        """
        graphs = []
        #Plot 50 indexes graph, if there are too many, this eases viewing, and we do not run into bash's arg_max limitations
        start=0
        end=0
        self.min_val = self.get_max()
        self.max_val = self.get_min()
        argmax = get_argmax()        
        #Plot until we run out of rows / columns
        if self.shape[0] > self.shape[1]:
            limit = self.shape[0]            
            limit_type = 'rows'
        else:
            limit = self.shape[1]
            limit_type = 'columns'
        #Split large data sets into small ones at index for graphing
        # If no split is given, disable splitting
        if self.split_interval == -1:
            interval = limit #Set splitting to the max of shape
        else:
            #If split is requested set interval to that 
            interval = min(self.split_interval, limit)
        counter = 0  #counter to limit iterations, in case any bug pops up (not the number of graphs)
        counter_limit = 20
        while limit > end and counter <= counter_limit:
            counter += 1
            if limit_type == 'columns':
                split_df = self.df.iloc[:,start:end+interval]
            else:
                split_df = self.df.iloc[start:end+interval,:]
            fig = Figure(figsize=(self.w, self.h))
            ax = fig.subplots()
            if self.kind == 'heatmap':
                annot = False
                if self.shape[1] < 1000:
                    annot = True
                if self.is_ts:
                    split_df = split_df.T
                sns.heatmap(split_df,
                            ax=ax, annot=True, fmt='2g', cmap=calmjet, linewidths=.1, linecolor="#DDDDDD",
                            vmax=self.max_val, vmin=self.min_val)
                ax.tick_params(axis='x', labelrotation=90)
                ax.tick_params(axis='y', labelrotation=0)
                if self.is_ts:
                    ax.xaxis.set_major_formatter(ticker.FuncFormatter(self.format_x))
            elif self.kind=='line':
                split_df.plot(ax=ax,
                              logx=False,
                              subplots=self.subplots,
                              kind=self.kind,
                              **kwargs)
                ax.tick_params(axis='x', labelrotation=90)
                ax.tick_params(axis='y', labelrotation=0)
            elif self.kind == 'scatter':
                split_df.plot(ax=ax,
                              subplots=self.subplots,
                              kind=self.kind,
                              x=self.x,
                              y=self.y[0])
            else:
                split_df.plot(ax=ax,
                              logx=False,
                              subplots=self.subplots,
                              kind=self.kind,
                              **kwargs)
                ax.tick_params(axis='x', labelrotation=90)
                ax.tick_params(axis='y', labelrotation=0)
            ax.set_title(self.title)                
            fig.set_tight_layout(tight=True)
            b64 = self.to_b64(fig, self.dpi)
            fig.clear()
            #If splitting is allowed, split the dataset
            if self.split_interval != -1 and len(b64) > argmax:
                if interval <= 5:
                    return []
                interval = max(5, interval - 10)
                start=0
                end=0
                continue
            graphs.append(b64)
            #Push start to new position
            start = end+interval
            #Push end to a new position
            end = end+interval
        return graphs
    
class Table(object):
    columns = []
    def __init__(self, src=None, delim=' ', heading=None, data=None,
                 max_fields=0, h1=False, fields=None,
                 missing_char='-'):
        self.delim = delim
        self.h1 = h1
        #if 1st line is not heading, use the heading provided, default is []
        if heading:
            self.heading = heading
        else:
            self.heading = []
        self.max_fields = max_fields
        self.missing_char = '-'
        #Check if fields are passed, we only need to filter data from those
        if isinstance(fields, str):
            self.fields = list(map(lambda x:int(x.strip()), fields.split(',')))
        else:
            self.fields = fields
        #If source is stdin or file
        if src == '-':
            self.src = '-'
            self.build_table_from_source()
        #Data is a list of lists; each row is a list; and in each row-list, the column items are in list
        else:
            self.data = data
            #Compute how many columns is needed
            if not self.max_fields:
                self.max_fields = max([len(i) for i in data])
            #Also fill the self.fields list
            self.fields = self.fields or list(range(self.max_fields))
            #If heading is give use it
            if h1:
                self.heading = self.data.pop(0)
        #Fill the headings, if there are some missing
        self.fill_heading()
        self.data, self.data_for_get_fields = tee(self.data, 2)
        #Keep a field index map of input and output
        self.field_map = dict(zip(self.fields, range(self.max_fields)))

    def add_row(self, row):
        self.data.append(row)
            
    def build_table_from_source(self):
        #Read the source
        self.src_data = self.get_input()
        #If the first line is heading, pop it out
        if self.h1:
            self.heading = next(self.src_data)
        #Otherwise check if heading is populated by user input
        elif self.heading == []:
            #If not use the fields numbers as hint for heading
            self.heading = ['col'+str(i) for i in self.fields]
        lines_for_max, lines_for_impute = tee(self.src_data, 2)
        #Compute max fields, needed for imputation of data
        if not self.max_fields:
            self.max_fields = max([len(i) for i in lines_for_max])
        #Impute missing data with missing_char
        self.data = self.impute_missing(lines_for_impute)
    
    def __repr__(self):
        return 'Table: delim="{}", heading={}, fields={}'.format(self.delim, self.heading, self.fields)
    
    def get_input(self):
        """Read data from src, with delim to split fields"""
        #For each line got from input
        for line in fileinput.input(self.src):
            #Strip the trailing newline character
            line = line.strip('\n')
            #Remove any leading spaces and split by the delim
            info = line.strip().split(self.delim)
            #If fields param is passed, Select only those fields
            if self.fields:
                #Using dict and enumerate to access fields by numbers
                filter_info = dict(enumerate(info))
                #If field exists give the result, else return None
                # This should also preserve the order of fields
                info = [filter_info.get(f) for f in self.fields]
            #If an empty string
            if not info or info == ['']:
                continue
            #Generate field list
            yield info

    def add_to_column(self, ):
        pass
    
    def impute_missing(self, lines):
        """Impute missing values on the dataset"""
        #Iterate over each line, find if the total fields in that line < max_fields
        # If it is, imput with the char field
        for line in lines:
            if len(line) < self.max_fields:
                need_fields = self.max_fields - len(line)
                line += [self.missing_char]*need_fields
            #Replace None values and ''
            #Strip any spaces within the column elements
            line = [i.strip() if i else self.missing_char for i in line]
            yield line

    def get_max_fieldlen(self):
        """Get the number of intended columns"""
        return self.max_fields

    def fill_heading(self):
        """Ensure heading for the expected columns have names, by choice or padding"""
        #If it is under or equal, pad it
        if len(self.heading)<=self.max_fields:
            #Pad heading if there is not enough
            self.heading += ['col{}'.format(i) for i in range(len(self.heading), self.max_fields)]
        #if it's over, trim it
        else:
            self.heading=self.heading[:self.max_fields]

    def get_fields(self, fieldN=None):
        """Return field name and data"""
        #If no fields is passed to get_fields
        if not fieldN:
            #Check if we have fields populated from object
            if self.fields:
                fieldN = self.fields
            #Otherwise generate a list using the max_fields
            else:
                fieldN = list(range(self.max_fields))
        field_data = []
        #self.fields is the field numbers of the input used to build the table
        #fieldN is the requested fields from (mapped to the input and not the table)
        #Mask generates a on-off list to filter only the fieldN data
        #ind_mask = [1 if i in fieldN else 0 for i in self.fields]
        field_d = {k: [self.heading[n], []] for n, k in enumerate(fieldN)}
        for row in self.data_for_get_fields:
            for n, k in enumerate(fieldN):
                field_d[k][1].append(row[n])
        #data = list(chain([list(compress(self.heading, ind_mask))],
        #[map(lambda x: list(compress(x, ind_mask)), self.data)]))
        return field_d

    def fast_ascii_table(self, heading_border=True, summary=False,
                         cell_width=15, repeat_heading=40):
        row_counter = 0
        row_f = ('{:<' + str(cell_width) + '} | ')*self.max_fields
        heading_f = row_f.format(*self.heading)
        print(heading_f)
        #If a heading border is needed
        #-1 is to accomodate for space after the last border
        heading_border = (len(heading_f)-1)*"-"
        print(heading_border)
        for i in self.data:
            row_counter += 1
            if row_counter%repeat_heading == 0:
                print(heading_border)
                print(heading_f)
                print(heading_border)                        
            print(row_f.format(*i), flush=True)

    def to_ascii_table(self, heading_border=True, summary=0, repeat_heading=40):
        """Convert heading and data into a fancy table"""
        row_counter = 0
        rows = list(self.data)
        if not rows:
            return None
        result = ''
        all_lines = [self.heading] + rows
        rowf_d = {}
        for n in range(len(self.heading)):
            #For each line
            # Convert the nth element in each line to string
            # Get the length of that element
            # Find the maximum length of the nth element among all the lines
            # Convert it to string format
            rowf_d[n] = str(max([len(str(i[n])) for i in all_lines]))
        row_f = '{:<' + rowf_d[0] + '} | '
        row_f += ''.join(['{:>' + rowf_d[v] +'} | ' for v in range(1, len(self.heading))])
        heading_f = row_f.format(*self.heading)
        result += heading_f + '\n'
        #If a heading border is needed
        if heading_border:
            #-1 is to accomodate for space after the last border
            heading_border = (len(heading_f)-1)*"-"
            result += heading_border + '\n'
        # Print until the penultimate row, the last few rows can be summary
        remaining_rows = 1+summary
        summary_rows = len(rows) - summary
        for i in rows[:summary_rows]:
            row_counter += 1
            if row_counter%repeat_heading == 0:
                 result += heading_border + '\n'
                 result += heading_f + '\n'
                 result += heading_border + '\n'
            result += row_f.format(*i)
            result += '\n'
        #If summary is needed, print a border
        if summary > 0:
            #-1 is to accomodate for space after the last border
            summary_border = (len(heading_f)-1)*"="
            result += summary_border+'\n'
        #Now pring the summary_rows
        for i in rows[summary_rows:]:
            result += row_f.format(*i)+'\n'
        return result

    def pipe(self, delim=' ', disable_heading=False):
        """Pipe result to stdut, with fields separated by delim"""
        #Print the heading first
        if not disable_heading:
            print(delim.join(self.heading))
        #Print the data next
        for line in self.data:
            print(delim.join(map(lambda x:str(x), line)), flush=True) 

    def tocsv(self):
        self.pipe(delim=',')

    def sort(self):
        pass

    def filterrows(self):
        pass

class ColumnTable(Table):
    """
    A object with multiple columns as its members
    """
    def __init__(self, data=[], name='Table', heading=[]):
        self.name = name
        self.cdata = data
        self.heading = heading
        self.data = []

    def add(self, data=[], cname='col'):
        if not isinstance(data, Column):
            data = Column(data)
        self.cdata.append(data.data)
        self.heading.append(data.name or cname)
        self.size = data.size

    def cols_to_rows(self):
        self.data = zip(*self.cdata)

    def to_ascii_table(self):
        if not self.data:
            self.cols_to_rows()
        return super().to_ascii_table()

    def tocsv(self, disable_heading=False):
        if not self.data:
            self.cols_to_rows()
        return super().pipe(delim=',', disable_heading=disable_heading)

    def pipe(self, delim=' ', disable_heading=False):
        if not self.data:
            self.cols_to_rows()
        return super().pipe(disable_heading=disable_heading)

    def __repr__(self):
        return 'ColumnTable: heading={}, name={}'.format(self.heading, self.name)
        
class Column(object):
    def __init__(self, data=None, name='Col', dtype=float, categorical=False):
        """
        Initialise the data necessary to create a column object

        data: list input of column data
        """
        #Set up the column with data from name
        self.name = name
        #Since Table usually passes generators, Convert into list since we will be reusing the data
        self.data = list(data)
        if categorical:
            self.categorical=True
        else:
            self.categorical=False
            #Keep a copy for categorical
            self.data_as_categorical = self.data
            #convert into float, so that imputation is easier            
            self.data = [self.astype(i, float) for i in self.data]
        self.size = len(self.data)
        self.agg_data = {}
        #For in-built functions
        self.fmap = {'add': self.__add__,
                     'divide': self.__div__,
                     'div': self.__div__,
                     'floordiv': self.__floordiv__,
                     'subtract': self.__sub__,
                     'sub': self.__sub__,
                     'multiply': self.__mul__,
                     'mul': self.__mul__,
                     'gt': self.__gt__,
                     'lt': self.__lt__,
                     'ge': self.__ge__,
                     'le': self.__le__,
                     'eq': self.__eq__,
                     'mod': self.__mod__,
                     'sample': self.f_sample,
                     'dummy': self.f_dummy
        }

    def set_column_name(self, name):
        self.name = name

    def __repr__(self):
        return "Column='{}', size={}".format(self.name, self.size)
    
    def __len__(self):
        return self.size

    def get_operand(self, operand):
        if isinstance(operand, Column):
            return operand.data
        elif isinstance(operand, list):
            return operand
        else:
            return list(repeat(operand, self.size))

    def apply_transform(self, f, other):
        """
        f -> function to run
        other -> params to pass into the function
        """
        if f:
            self.data = self.data
        #Hack for subtract_from        
        if f == 'subtract_from':
            self.data = self.__neg__()
            f = 'subtract'
        #If the function is in a list of known internal functions
        if f in self.fmap:
            self.data = list(self.fmap[f](other))
        #If it's not a defined function internally
        else:
            #If function is defined under the global namespace, eval the function
            if f in globals():
                f = eval(f)
                self.data = f(self.data, other)
            else:
                #Otherwise just let the user know
                print("{} does not exist".format(f))

    def transform(self, f, other):
        #Create a mapping for function and data to apply on
        # a-> is the data input
        # b-> parameter for the function to use on a
        return starmap(lambda a,b:f(a,b) if (a and b)!=None else nan, zip(self.data,
                                                                          self.get_operand(other)))

    def astype(self, x, f):
        try: return f(x)
        except (TypeError, ValueError): return nan

    def __add__(self, other):
        return self.transform(lambda a,b: a+b, other)

    def __sub__(self, other):
        return self.transform(lambda a,b: a-b, other)

    def __mul__(self, other):
        return self.transform(lambda a,b: a*b, other)

    def __div__(self, other):
        return self.transform(lambda a,b: a/b, other)

    def __floordiv__(self, other):
        return self.transform(lambda a,b: a//b, other)

    def __mod__(self, other):
        return self.transform(lambda a,b: a%b, other)

    def f_sample(self, other):
        return self.transform(lambda a,b: a-(a%b), other)

    def __neg__(self):
        return self.__mul__(-1)

    def __eq__(self, other):
        return self.transform(lambda a,b: 1 if a==b else 0, other)

    def __lt__(self, other):
        return self.transform(lambda a,b: 1 if a<b else 0, other)

    def __gt__(self, other):
        return self.transform(lambda a,b: 1 if a>b else 0, other)

    def __le__(self, other):
        return self.transform(lambda a,b: 1 if a<=b else 0, other)

    def __ge__(self, other):
        return self.transform(lambda a,b: 1 if a<=b else 0, other)

    def __ne__(self, other):
        return self.transform(lambda a,b: 1 if a!=b else 0, other)

    def __iter__(self):
        return iter(self.data)

    def f_dummy(self, other):
        """Return the param value for every data"""
        return self.transform(lambda a,b: b, other)

    def filter_nans(self, data):
        #equality fails
        #https://towardsdatascience.com/navigating-the-hell-of-nans-in-python-71b12558895b
        return filter(lambda x:x is not nan, data)

    def aggregate(self, f):
        """Aggregate functions"""
        if not self.data:
            self.agg_data[f] = None
            return
        if f == 'first': self.agg_data[f] = self.data[0]
        if f == 'last': self.agg_data[f] = self.data[-1]
        if f == 'max': self.agg_data[f] = max(self.data)
        if f == 'min': self.agg_data[f] = min(self.data)
        if f == 'sum':  self.agg_data[f] = sum(self.data)
        if f == 'count': self.agg_data[f] = len(self.data)
        if f in ('mean', 'average', 'avg'): self.agg_data[f] = round(self.data, 2)
        if f in ('median', 'p50'): self.agg_data[f] = stats.median(self.data)
        if f in ('stdev'): self.agg_data[f] = round(stats.pstdev(self.data), 2)

    def summary(self):
        result = []
        #Filter out nans for handling continous data
        data_no_nans = sorted(self.filter_nans(self.data))
        size = len(data_no_nans)
        dtype = None
        if size > 0:
            dtype = 'continous'
            #The necessary summary data
            result.append(['count', size])
            result.append(['min', min(data_no_nans)])
            result.append(['max', max(data_no_nans)])
            result.append(['mean', round(sum(data_no_nans)/size, 2)])
            result.append(['stddev', round(stats.pstdev(data_no_nans), 2)])
            pct = [5, 25, 50, 75, 90, 95, 99]
            for p in filter(lambda x:x<=100 and x>=0, pct):
                #Compute the index for the percentile
                # Reduce by 1 because of the 0 indexing
                # using ceil to find the value when it's not an integer
                ind = ceil((p*size)/100)
                #Add boundaries on the result to range from 0->highest index
                ind = max(0, min(ind, size-1))
                result.append(['{}p'.format(p), data_no_nans[ind]])
        else:
            dtype = 'categorical'
            size = len(self.data_as_categorical)
            Ctr = Counter(self.data_as_categorical)
            result.append(['unique', len(Ctr.keys())])
            result.append(['count', size])
            cumsum = 0
            topN = []
            for x in Ctr.most_common()[:5]:
                cumsum += x[1]*100/size
                topN.append(round(cumsum, 2))
            topN += ['-']*(5-len(topN))
            for n, i in enumerate(topN, 1):
                result.append(['top{}'.format(n), '{}'.format(i)])
            result.append(['most', Ctr.most_common()[0][0]])
            result.append(['least', Ctr.most_common()[-1][0]])

        return dtype, result
    
class Pivot(Table):
    def __init__(self, row_k=1, col_k=2, val_k=None, f=None, summary=False,
                 src=None, delim=' ', heading=None, data=None,
                 max_fields=0, h1=False, fields=None,
                 missing_char='-', summaryf=None, rowsummary=True, colsummary=True):
        """
        - Get the necessary arguments for Pivot table, rest are passed as kwargs for Table
        - The defaults are set in line with the results of the commong output of 
        `sort | uniq -c` ; 
        - count is the first field, so it is value
        - row is the second field, because it is usually the time stamp
        - column is the third field, because it is the next key
        """
        self.row_k = row_k
        self.col_k = col_k
        self.rowsummary = rowsummary
        self.colsummary = colsummary
        #If val_k is passed we will need to use that column
        if val_k != None:
            self.val_k = val_k
        #Otherwise we need to use some statistical summary
        else:
            pass
        self.aggfunc = f
        self.summary = summary
        self.summaryfunc = []
        if summary:
            self.summaryfunc = [self.aggfunc]
        if summaryf:
            self.summaryfunc += summaryf
        self.summarydata = {'row':{'heading':[], 'data':[]},
                            'col':{'heading':[], 'data':[]}}
        #Get the function initialised with the super init
        super().__init__(src, delim, heading, data,
                         max_fields, h1, fields,
                         missing_char)

    def pivot2(self):
        """Pivot2 is for grouping and summarising data for row ind key using data from valueind.
        It does not need colind
        """
        rp = 0
        cp = 1
        vp = 2
        pivot_d = defaultdict(list)
        row_v = set()
        for d in self.data:
            #Get the row index, if it's None, use nan
            row_v.add(d[rp] or nan)
            #Get the values in a list for each rowindex, colindex, if it's None use 0
            pivot_d[d[rp]].append(d[vp] or 0)
        #Making row_v into a list and sort it
        row_v = sorted(list(row_v))
        pivot_data = []
        for row in row_v:
            cells = []
            for f in self.summaryfunc:
                cell = f_aggfunc(pivot_d[row], f, need_sort=True) or self.missing_char
                cells.append(cell)
            pivot_data.append([row] + cells)
        self.heading = ['({})'.format(self.heading[self.val_k])]
        self.heading += self.summaryfunc
        self.data = pivot_data
        self.pivotdata = pivot_data

    def pivot(self):
        """Pivot on row index and column index with an aggregation function applied on value index"""
        rp = 0 #row pointer as the result will be got from the table in the order r, c, v
        cp = 1
        vp = 2
        row_v = set()
        col_v = set()
        #Pivot_d
        # 'Row1' : { 'Col1': [value_list], 'Col2': [value_list] },
        # 'Row2' : { 'Col1': [value_list], 'Col2': ... }
        pivot_d = defaultdict(dict)
        for d in self.data:
            #Get the row index, if it's None, use nan
            row_v.add(d[rp] or nan)
            #Get the column index, if it's None, use nan
            col_v.add(d[cp] or nan)
            #Get the values in a list for each rowindex, colindex, if it's None use 0
            if not pivot_d[d[rp]].get(d[cp]):
                pivot_d[d[rp]][d[cp]] = []
            pivot_d[d[rp]][d[cp]].append(d[vp] or 0)
        #Making col_v into a list and sort it
        col_v = sorted(list(col_v))
        row_v = sorted(list(row_v))
        #heading item for output made from pivot_heading and columns
        pivot_data = []
        for row in row_v:
            col_r = []
            for col in col_v:
                data = sorted(pivot_d[row].get(col, []))
                cell = f_aggfunc(data, self.aggfunc) or self.missing_char
                col_r.append(cell)
            pivot_data.append([row, *col_r])
        self.data = pivot_data
        self.pivotdata = pivot_data
        #If summary is needed, running on the resulting pivot table
        #Rest heading, Construct pivot heading using the parent Table's heading
        self.row_v = row_v
        self.col_v = col_v
        self.heading = ['{}({})'.format(self.aggfunc, self.heading[self.val_k])]
        self.heading += self.col_v
        self.pivotheading = self.heading
        #Reset max_fields
        self.max_fields = len(self.heading)        
        summary_data = pivot_data.copy()
        summary_heading = self.heading.copy()
        for func in self.summaryfunc:
            self.add_summary(summary_data, summary_heading, func,
                             rowsummary=self.rowsummary, colsummary=self.colsummary)
            self.data[-1] += ['*']*len(self.summaryfunc)

    def add_summary(self, summary_data, summary_heading, func,
                    rowsummary=True,
                    colsummary=True):
        """Add a summary column for the resulting pivot table.
        2 options, summarise the pivoted table's rows and/or summarise the pivoted
        table's columns"""
        if rowsummary:
            self.summarydata['row']['heading'].append(':RSummary({}):'.format(func))
            self.heading.append(':RSummary({}):'.format(func))        
            #Summary for each row_index, using the pivot result
            # row[1:]->First row is index
            for n, row in enumerate(summary_data):
                #To list as the f_aggfunc for first/last cannot handle filter
                # If result is None, use missing char
                data = list(filter(lambda x:x!=self.missing_char,
                                     row[1:]))
                result = f_aggfunc(data, func, need_sort=True)
                if result == None:
                    result = self.missing_char
                sum_row = result
                self.data[n].append(sum_row)
                self.summarydata['row']['data'].append(sum_row)

        if colsummary:
            self.summarydata['col']['heading'].append(':CSummary({}):'.format(func))
            sum_col=[':CSummary({}):'.format(func)]
            #Summary for each col_index, using the pivot result
            # Appended to the table
            # The last value will be a summary of the summaries of the row index
            for n, col in enumerate(summary_heading[1:], start=1):
            #To list as the f_aggfunc for first/last cannot handle filter
            # If result is none, use missing_char
                data=list(filter(lambda x:x!=self.missing_char,
                                 [i[n] for i in summary_data]))
                result = f_aggfunc(data, func, need_sort=True)
                #If result is None, replace it with missing_char
                if result == None:
                    result = self.missing_char
                sum_col.append(result)
            self.data.append(sum_col)
            self.summarydata['col']['data'].append(sum_col)

    def __repr__(self):
        return 'Pivot: delim="{}", heading={}, fields={}, aggfunc={}'.format(self.delim, self.heading, self.fields, self.aggfunc)

    def to_ascii_table(self):
        #Intialise summary_rows as 0
        summary_rows = 0
        #If we have column key, that means, we are using the proper pivot
        # get the summary rows as the length of the summary function list
        # AND colsummary is TRUE
        if self.col_k and self.colsummary:
            summary_rows = len(self.summaryfunc)
        return super().to_ascii_table(heading_border=True,
                                      summary=summary_rows)

    def get_pivotdata(self):
        return {'data': self.pivotdata, 'heading': self.pivotheading}

    def get_summarydata(self):
        return self.summarydata


import argparse
from math import nan, ceil, floor, isnan
from collections import Counter
import argparse
import sys
from itertools import chain, zip_longest

g_format='kind:x:y:hue:split_interval:is_ts:subplots'
g_ex='bar:1:0:2:10:False:False --> Plot a bar chart(kind), with x-axis as column 1(x), y-axis from column 0(x), and group data by column2(hue). Split the graphs such that each graph has 10 values(split_interval) in x-axis, x-axis is not of time format(is_ts=False), Plot all data in 1 chart(subplots=False)'

if sys.getfilesystemencoding() == 'utf-8':
    bar_char = '█'
else:
    bar_char = 'o'

def parse_args():
    parser = argparse.ArgumentParser(description="Pass arguments to matrix creator. By default data is tabulated in a simple readable way with boundaries, if pivot flag is used, data is pivoted based on the pivot options passed")

    #Dictionary with options and info for reusing multiple times
    args_d = {'fields': [['-f', '--fields'], {'type': str,
                                              'help': "Field numbers to show in result, (numbers separated by comma or range separated by dash). Default is to show all.",
                                              'default': None,
                                              'metavar': '1,2,3,... or 1-3,5,8-10'}],
              'tocsv': [['--tocsv'], {'action': 'store_true',
                                      'help': "write output as a csv to terminal",
                                      'default': None}],
              'delim': [['-d', '--delim'], {'type': str,
                                            'help': "Delimiter to split the input fields. Default is space '%(default)s'",
                                            'default': None,
                                            'metavar': 'delimiter',
                                            'dest': "delim"}],
              'pipe': [['--pipe'], {'action': 'store_true',
                                    'help': "Pipe data to output"}],
              'heading': [['--heading'], {'type': str,
                                          'help': "Custom heading to use, separate headings with comma (,). Missing ones will have colN ... N->field number", 'default': None}],
              'h1': [['-h1'], {'action': 'store_true',
                               'help': "Indicates that the first line is a heading",
                               'default': False}],
              'graph': [['--graph'], {'type': str,
                                      'help': "Graph the data (experimental), information is of format - {}. ex: {}. If format is not passed, a default format is assumed".format(g_format, g_ex),
                                      'metavar': 'GRAPH_OPTION_FORMAT',
                                      'default': False,
                                      'nargs': '?'
                                      }],
              'noheading': [['--noheading'], {'action': 'store_true',
                                              'help': 'Disables printing of heading on output when used with pipe/tocsv options. Useful if the data needs to passed into sort,uniq command',
                                              'default': False}],
              'notable': [['--notable'], {'action': 'store_true',
                                              'help': 'Disables printing of data on output. Useful if the tograph is used and the data is not needed.',
                                              'default': False}],
              'fast': [['--fast'],
                       {'action':'store_true',
                        'help': 'Attempts to be faster in producing the ascii table output, by pre-assuming cell widths of table. Use --width to set custom cell widths.'}],
              'rich': [['--rich'],
                     {'action':'store_true',
                              'help': 'fancy table printing, only works if the rich python module is installed (Does not install by default).'}]}
    #Set of common options to all
    commongroup = parser.add_argument_group("Common options")
    commongroup.add_argument('-h1', action='store_true', help="Indicates that the first line is a heading", default=False,
                             dest="com_h1")
    commongroup.add_argument('-d', '--delim', type=str, help="Delimiter to split the input fields. Default is space '%(default)s'",
                             default=' ',
                             metavar='delimiter',
                             dest="com_delim")
    commongroup.add_argument(*args_d['fields'][0], **args_d['fields'][1])
    commongroup.add_argument('--fast', action='store_true',
                             help='Attempts to be faster in producing the ascii table output, by pre-assuming cell widths of table. Use --width to set custom cell widths.')

    #This set is to reflect tablegroup's options so that it can run as default
    parser.add_argument('--pipe', action='store_true', help=argparse.SUPPRESS)
    parser.add_argument('--tocsv', action='store_true', help=argparse.SUPPRESS, default=None)
    parser.add_argument('--graph', type=str, help=argparse.SUPPRESS)
    #Do not include list inputs here https://stackoverflow.com/questions/35898944/python-subparser-parsing-with-nargs
    parser.add_argument('--heading', type=str, help=argparse.SUPPRESS, default=None)
    
    actions = parser.add_subparsers(title="Available actions (Use -h after action for more options)", metavar='', dest='action')

    #table; options
    tablegroup = actions.add_parser(name='table', help="Tabulate the input fields (Default)",
                                    description="Pretty print the input data as tables. Columns can be chosen to print. By default, all columns are printed")
    for i in ['fields', 'tocsv', 'delim', 'pipe', 'heading', 'h1', 'noheading', 'notable', 'graph', 'fast', 'rich']:
        tablegroup.add_argument(*args_d[i][0], **args_d[i][1])        

    #summary: options
    aggregategroup = actions.add_parser(name='summary', help="Similar to pandas dataframe describe(), gives a statistical summary of the result, All values are treated as continous data")
    for i in ['delim', 'h1', 'heading', 'rich']:
        aggregategroup.add_argument(*args_d[i][0], **args_d[i][1])

    #hist: options
    histgroup = actions.add_parser(name='hist', help="Get the histogram of the input fields",
                                   description="If bins, size, count is provided. Bins is preferred over size and size over count. If none of them is provided, default is to use count=40")
    for i in ['fields', 'delim', 'heading', 'h1', 'notable', 'rich', 'graph']:
        histgroup.add_argument(*args_d[i][0], **args_d[i][1])
    histgroup.add_argument('--min', type=int, help="the lowest of the bins. Default is the minimum of the data.", metavar='N')
    histgroup.add_argument('--max', type=int, help="the highest of the bins, highest value in set. Default is the maximum of the data.", metavar='N')
    histgroup.add_argument("--bins", type=int, nargs="+",
                           help="Specify the bins manually separated by space. They act as the upper edge of the bin. The lower edge is the previous bin or the lowest-1 value. Has to be intergers",
                           default=[])
    histgroup.add_argument("--size", type=int, help="Size of each bins.", metavar='N')
    histgroup.add_argument('--count', type=int, help="Count of histogram bins to have. Default is %(default)s.", default=20, metavar='N')
    histgroup.add_argument('--summary', action='store_true',
                           help="Add statistical summary data",
                           default=False)

    #pivot: options
    pivotgroup = actions.add_parser(name='pivot', help="Pivot the input data",
                                    description="Pivot the input data by creating row and column indices and computing the value for each using input fields.")
    for i in ['delim', 'heading', 'h1', 'notable', 'rich', 'graph', 'tocsv', 'pipe']:
        pivotgroup.add_argument(*args_d[i][0], **args_d[i][1])
    pivotgroup.add_argument('-r', '--rowind', type=int, help="Position of the data that needs to be used as row index. Starts from 0",
                            metavar='N',
                            default=None)
    pivotgroup.add_argument('-c', '--columnind', type=int, help="Position of the data that needs to be used as column index. Starts from 0.", default=None, metavar='N')
    pivotgroup.add_argument('-v', '--valueind', type=int, help="Position of data that needs to be added as value to use on the cell. Starts from 0.", default=None, metavar='N')
    pivotgroup.add_argument('--aggfunc', type=str, help="Agg function to use if there are multiple values for the row x column combination. Default is %(default)s",
                            choices=['first', 'last', 'concat', 'max', 'min', 'sum', 'count', 'mean', 'median', 'stdev'],
                            default='first')
    pivotgroup.add_argument('--summary', action='store_true',
                            help="Add a summary column using the same agg function, the summary is on the resulting cells with the aggfunc applied on them.",
                            default=False)
    pivotgroup.add_argument('--summaryf', type=str,
                            help="Running summary functions on the results, use this if you want multiple summaries",
                            default=None)
    pivotgroup.add_argument('--rowsummary', action='store_true',
                            help="Only print the row summary, default is to print both column and row summaries",
                            default=False)
    pivotgroup.add_argument('--colsummary', action='store_true',
                            help="Only print the column summary, default is to print both column and row summaries",
                            default=False)
    
    #transform: options
    transform_function_l = ['add', 'divide', 'div', 'floordiv', 'subtract', 'sub',
                            'multiply', 'mul', 'gt', 'lt', 'ge', 'le', 'eq', 'mod',
                            'dummy', 'sample']
    transformgroup = actions.add_parser(name='transform', help="Transform columns by running functions on them")
    for i in ['delim', 'heading', 'h1', 'noheading', 'tocsv', 'pipe', 'fields']:
        transformgroup.add_argument(*args_d[i][0], **args_d[i][1])
    transformgroup.add_argument('--function', action="append", help="function to run on the field. 1 field and one1 action is supported. Format is fieldNumber:function:arguments. Available functions are {}. Custom functions can also be used from eda_custom_functions.py".format(transform_function_l), metavar="format")

    args = vars(parser.parse_args())
    # If no options are provided, print the help
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(-1)
    return args


if __name__=='__main__':
    #Get all the arguments
    args = parse_args()
    ##Common fields, prefer action's option first otherwise use the common option
    fields = args.get('fields') or args.get('com_fields')
    h1 = args.get('h1') or args.get('com_h1')
    delim = args.get('delim') or args.get('com_delim')
    #If --graph is not used, it will have the default False
    # If only the --graph is used, it will have None
    # If --graph 'g_options' , it will be a string
    tograph = args.get('graph')
    if tograph != False:
        #Return g_options if passed otherwise true (if g_options is None)
        tograph = tograph or True
    tocsv = args.get('tocsv') or args.get('com_tocsv')
    pipe = args.get('pipe') or args.get('com_pipe')
    rich = args.get('rich') or args.get('com_rich')
    heading = args.get('heading') or args.get('com_heading')
    #If heading is provided, it will be comma separated, split at commas
    if heading:
        heading = heading.split(',')
    noheading = args.get('noheading') or args.get('com_noheading')
    notable = args.get('notable') or args.get('com_notable')

    #Action to do
    action = args.get('action') 
    #Table fields
    fast = args.get('fast')
    #Hist fields
    minv = args.get('minv')
    maxv = args.get('maxv')
    bins = args.get('bins')
    size = args.get('size')
    count = args.get('count')
    summary = args.get('summary')
    
    #Pivot fields
    rowind = args.get('rowind')
    columnind = args.get('columnind')
    valueind = args.get('valueind')
    aggfunc = args.get('aggfunc')
    summary = args.get('summary')
    summaryf = args.get('summaryf')
    rowsummary = args.get('rowsummary')
    colsummary = args.get('colsummary')
    
    #Transform fields
    function = args.get('function')

    #Handle field format input
    if fields:
        fields = get_fields(fields)

    #Getting the fields based on the actions
    if action == 'pivot':
        #If all the indices are not given, it is likely a standard sort | uniq -c result
        if columnind == None and rowind == None and valueind == None:
            rowind=1
            columnind=2
            valueind=0
        fields = list([rowind, columnind, valueind])
    if action == 'transform':
        t_list = get_transform_req(function)
        f1 = chain.from_iterable([f['fields'] for f in t_list])
        f2 = [f['params'] for f in t_list if f['is_field']]
        #Get the fields that are used in the transform
        f_fields = list(set(chain(f1, f2)))
        #If fields is passed, append it to that that
        if fields:
            fields = fields + f_fields
        #else fields = the transform fields
        else:
            fields = f_fields
    #If there were none, use the special empty list
    if fields == None:
        fields = []
    fields = get_uniq_fields(fields)

    #Handle graphing related steps
    # If tograph is False, then the option was not selected
    # If action is hist, no need to do anything, hist has it's own g_options setup
    g_options = {}
    if tograph != False and action!='hist':
        if isinstance(tograph, (str)):
            g_options = dict(zip_longest(g_format.split(':'), tograph.split(':')))
            if g_options['y']:
                g_options['y'] = get_fields(g_options['y'])
            else:
                g_options['y'] = [1]
            if g_options['x']:
                g_options['x'] = int(g_options['x'])
            else:
                g_options['x'] = 0
            for i in ['is_ts', 'subplots']:
                if g_options[i] == 'True':
                    g_options[i] = True
                else:
                    g_options[i] = False

    #Creating the table object
    if action == 'pivot':
        if summaryf:
            summaryf = [f.strip() for f in summaryf.split(',')]
        #Only change rowsummary/colsummary, when it is different, use xor to check that
        if rowsummary ^ colsummary:
            rowsummary = rowsummary
            colsummary = colsummary
        #otherwise both are true
        else:
            rowsummary = True
            colsummary = True
        T = Pivot(src='-', delim=delim, fields=fields, h1=h1,
                  row_k=rowind, col_k=columnind,
                  val_k=valueind, f=aggfunc, summary=summary,
                  heading=heading, summaryf=summaryf, rowsummary=rowsummary,
                  colsummary=colsummary)
    else:
        T = Table(src='-', delim=delim, fields=fields, h1=h1, heading=heading)

    #Actions to do
    #Pivoting
    if action == 'pivot':
        #If only column ind is not given, it is pivot and summary for each row combination, so use pivot2
        if not columnind:
            T.pivot2()
        #Else standard pivoting using pivot
        else:
            T.pivot()
        if not notable:
            if fast:
                print(T.fast_ascii_table())
            elif rich:
                out = rich_print_table(T.data, T.heading)
                print(out)
            else:
                print(T.to_ascii_table())
        #Get pivotdata for graphing, no summaries
        output = T.get_pivotdata()
        #If graphing is requested
        if tograph:
            is_ts = False
            if g_options:
                is_ts = g_options['is_ts'] 
            y = range(1, len(output['data'][0]))
            title = "Heatmap of pivot result"
            g_options = {'kind': 'heatmap', 'x': 0, 'y': y,
                         'split_interval': 50,
                         'is_ts': is_ts, 'subplots': False, 'title': title}
            g=Graph(data=output['data'], heading=output['heading'], **g_options)
            graphs = g.plot()
            for fig in graphs:
                imgcat(fig)

    #Simple ASCII Table
    if not action or action == 'table':
        if not notable:
            if pipe:
                T.pipe(disable_heading=noheading)
            elif tocsv:
                T.tocsv(disable_heading=noheading)
            elif fast:
                #Handle BrokenPipeError
                # https://stackoverflow.com/a/26738736
                try:
                    print(T.fast_ascii_table(), flush=True)
                except (BrokenPipeError):
                    pass
                sys.stderr.close()
            else:
                if rich:
                    out = rich_print_table(T.data, T.heading)
                    print(out)
                    if out == None:
                        print(T.to_ascii_table(), flush=True)
                else:
                    print(T.to_ascii_table(), flush=True)        
        if tograph:
            title = "Graph"
            #If the x-axis for the graph is passed
            if g_options.get('x') != None:
                #Get the fields requested (this will follow the order of the input data)
                x = g_options['x']
                #Get the mapping of the output of table object, this is needed for graphs
                g_options['x'] = T.field_map[x]
            #If it is not provided, then we will use the default order
            else:
                x = 0
                g_options['x'] = 0
            #Same action with y
            if g_options.get('y') != None:
                y = g_options['y']
                g_options['y'] = [T.field_map[f] for f in y]
            else:
                y = [1]
                g_options['y'] = 1
            fields = [x, *y]
            data = T.get_fields(fields)
            df_data = {v[0]:v[1] for _, v in data.items()}
            if not g_options:
                g_options = {'kind': 'line', 'x': 0, 'y': [1],
                             'split_interval': -1, 'is_ts': False,
                             'subplots': False, 'title': title}
            #If scatter plot disable splitting
            if g_options['kind'] == 'scatter':
                g_options['split_interval'] = -1
            g=Graph(data=df_data, **g_options)
            graphs = g.plot()
            for fig in graphs:
                imgcat(fig)

    #Summarising
    if action == 'summary':
        data = T.get_fields(fields)
        result = []
        summary_data_continous = {'heading': [], 'data': []}
        summary_data_categorical = {'heading': [], 'data': []}
        for k, v in data.items():
            C = Column(data=v[1], name=v[0])
            dtype, data = C.summary()
            Csum = transpose_rows(data)
            if not Csum:
                continue
            if dtype == 'continous':
                summary_data_continous['heading'] = ['field'] + Csum[0]
                summary_data_continous['data'].append([C.name] + Csum[1])
            else:
                summary_data_categorical['heading'] = ['field'] + Csum[0]
                summary_data_categorical['data'].append([C.name] + Csum[1])                
        if summary_data_continous['data']:
            Tsum = Table(data=summary_data_continous['data'],
                         heading=summary_data_continous['heading'])
            if rich:
                print(rich_print_table(summary_data_continous['data'],
                                   summary_data_continous['heading']))
            else:
                print(Tsum.to_ascii_table(repeat_heading=30))
        if summary_data_categorical['data']:
            Tsum = Table(data=summary_data_categorical['data'],
                         heading=summary_data_categorical['heading'])
            if rich:
                print(rich_print_table(summary_data_categorical['data'],
                                   summary_data_categorical['heading']))
            else:
                print(Tsum.to_ascii_table(repeat_heading=30))
        

    #Transforming
    if action == 'transform':
        pTable = T.get_fields(fields)
        tTable = ColumnTable(name='Transformed')
        for pk, pv in pTable.items():
            pCol = Column(pv[1], name=pv[0], categorical=True)
            tTable.add(pCol)
        for t in t_list:
            #Get the column name, or use dummy name
            name = t.get('alias', '{}({},{})'.format(t['func'],
                                                     'a', 'b'))
            #If param is a number, use it, if it's a field, get the Column object of that field
            t['params'] = t['params'] if not t['is_field'] else Column(pTable[t['fields'][0]][1], name=pTable[t['fields'][0]][0])
            #Hack to check if it is a categorical column
            #Check if 'params' can be converted into float
            categorical = False
            if not t['is_field']:            
                try:
                    #If it can be, then its likely a numerical column
                    params = float(t['params'])
                except ValueError:
                    #Else its intended to be a categorical column
                    params = t['params']
                    categorical = True
            else:
                params = t['params']
            #Get the data for the fields where transform needs to be applied
            # and create a column object, name it using alias key
            if isinstance(t['params'], (Column)):
                param_name = t['params'].name
            else:
                param_name = t['params']
            if t['alias'] == '':
                t['alias'] = '{}({},{})'.format(t['func'],
                                                 pTable[t['fields'][0]][0],
                                                 param_name)
            tCol = Column(pTable[t['fields'][0]][1],
                          name=t['alias'],
                          categorical=categorical)
            #Apply transform
            tCol.apply_transform(t['func'],
                                 params)
            #If there is a chain function, follow the same process as above one-after-another
            if t['chain']:
                for c in t['chain']:
                    c['params'] = c['params'] if not c['is_field'] else Column(pTable[c['fields'][0]][1])
                    tCol.apply_transform(c['func'], c['params'])
            tTable.add(tCol)
        if pipe:
            tTable.pipe(disable_heading=noheading)
        elif tocsv:
            tTable.tocsv(disable_heading=noheading)
        else:
            print(tTable.to_ascii_table())

    #Histogram
    if action == 'hist':
        #Gather all arguments, passing it as **kwargs to functions
        kwargs = {'minv': minv, 'maxv': maxv, 'count': count,
                  'bin_size': size, 'bins': bins}
        #Get only the data from the fields
        Tdata = T.get_fields(fields)
        asciigraph = True
        #k->field number; v -> [column_name, [values]]
        for Ck, Cv in Tdata.items():
            #The column name
            name = Cv[0]
            C = Column(data=Cv[1], name=name)
            data = list(filter(lambda x: not isnan(x), C.data))
            heading=['bins', 'count', 'share%']
            #Categorical columns will return nan, so data is [], so we will use counter
            if not data:
                #Initialise a list to collect the rows
                hist_table = []
                counter = Counter(Cv[1])
                for k, v in counter.items():
                    share = round(int(v)/len(Cv[1])*100,2)
                    row = [k, v, share]
                    if asciigraph:
                        width = 20
                        barlength = int((width*share)/100)
                        bar = bar_char*barlength + ' '*(width-barlength)
                        row.append(bar)
                    hist_table.append(row)
            else:
                hist_table = []
                total_count = len(data)
                bin_d = get_result(data, **kwargs)
                if not bin_d:
                    continue
                #Start is taken as the minimum bin-1
                i_old=sorted(bin_d)[0]-1
                field_len = max([len(str(k)) for k in bin_d.keys()])*2 + 3
                hist_table = []
                for i in sorted(bin_d.keys()):
                    bin_s = "({}-{}]".format(i_old, i)
                    share = round(bin_d[i]*100/total_count)
                    row = [bin_s, bin_d[i], share]
                    if asciigraph:
                        width = 20
                        barlength = int((width*share)/100)
                        bar = bar_char*barlength + ' '*(width-barlength)
                        row.append(bar)
                    hist_table.append(row)
                    i_old = i
            title = "Histogram of {}".format(name)
            if asciigraph:
                heading.append("histogram")
            if not notable:
                if rich:
                    print(rich_print_table(data=hist_table, heading=heading, title=title, justify={0: 'left', 3: 'left'}))
                else:
                    print(title)
                    #Print the table
                    hT = Table(data=hist_table, heading=heading)
                    print(hT.to_ascii_table())
            #If graphing is requested
            if tograph:
                output = {'heading': heading, 'data': hist_table}
                g_options = {'kind': 'bar', 'x': '0', 'y': '1', 'subplots': True, 'split_interval': -1, 'vert': False, 'title': title}
                g=Graph(data=hist_table, heading=heading, **g_options)
                graphs = g.plot()
                for fig in graphs:
                    imgcat(fig)


                    
                

